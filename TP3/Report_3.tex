\documentclass[a4paper, 12pt]{article}
 
\usepackage[applemac]{inputenc}
\usepackage{graphicx}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{float}
%\usepackage{fullpage}


 
\begin{document}
 
\title{TL Programming Assignment 3}
\author{Mathurin \textsc{Massias} \and Clément \textsc{Nicolle}}
\date{\today} 
 
\maketitle

\section{Full information, partial information}
\hspace{-6mm}\textbf{1):} It is a problem with full information since $R_A$ is known by $A$. A problem with partial information would be %TODO


\section{EWF and EXP3 versus an oblivious adversary}
\underline{Question 1:} We choose arbitrarily as a sequence for $B$ : $(2, 2, 1, 2, 2, 1, 2, 2, 1\ldots)$ 
\\[3mm]We take $\eta = \sqrt{\frac{N \mathrm{log} N}{n(e-1)}}$ (formula from the class, here $N = 2$) and $\eta = 0.04$. For EXP3 we also take $\beta = \eta$.
\\We get the following regret curves :


\begin{figure}[H]
\centering
\noindent\includegraphics[scale=0.6]{Q1-regrets.png}
\caption{Regrets for EXP3 and EWF strategy}
\end{figure}



\section{EXP3 versus EXP3 : Nash Equilibrium}
\underline{Question 2 :}

\begin{figure}[H]
\centering
\noindent\includegraphics[scale=0.6]{Q2-probas.png}
\caption{Convergence of empirical probabilities at horizon 500}
\end{figure}

\begin{figure}[H]
\centering
\noindent\includegraphics[scale=0.6]{Q2-rewards.png}
\caption{Convergence of the mean reward for player A}
\end{figure}

Even if the horizon may be not enough large, we see on Figure 2 that the probabilities seem to converge towards the Nash equilibrium $(p_a^*, p_b^*)$. Furthermore, we see more easily on the graph that the empirical mean for player A converges towards the value of the game, which is around 0,4.\\

If we increase $\eta$, we state that the convergence is faster, but the equilibrium changes. For example, for $\eta=\beta=5$ A would always chose 2 from a certain point, and B 1.

\begin{figure}[H]
	\centering
	\noindent\includegraphics[scale=0.6]{Q2-probas-eta05.png}
	\caption{Convergence of empirical probabilities at horizon 500 for $\eta=\beta=0,5$}
\end{figure}

\begin{figure}[H]
	\centering
	\noindent\includegraphics[scale=0.6]{Q2-rewards-eta05.png}
	\caption{Convergence of the mean reward for player A for $\eta=\beta=0,5$}
\end{figure}

\begin{figure}[H]
	\centering
	\noindent\includegraphics[scale=0.6]{Q2-probas-eta5.png}
	\caption{Convergence of empirical probabilities at horizon 500 for $\eta=\beta=5$}
\end{figure}

\begin{figure}[H]
	\centering
	\noindent\includegraphics[scale=0.6]{Q2-rewards-eta5.png}
	\caption{Convergence of the mean reward for player A for $\eta=\beta=5$}
\end{figure}

\section{Stochastic bandit or adversarial bandit ?}

The original problem is the following :
\begin{verbatim}
Arm1=armBernoulli(0.2);
Arm2=armBernoulli(0.4);
Arm3=armBernoulli(0.5);

MAB={Arm1,Arm2,Arm3};
\end{verbatim}

We obtain these regret curves :

\begin{figure}[H]
	\centering
	\noindent\includegraphics[scale=0.7]{Q3-regrets-originalpb.png}
	\caption{Regret curves for Thompson Sampling and Exp3 strategies on original problem}
\end{figure}

We see that Thompson Sampling does much better than Exp3 strategy.
\\
We computed it for another problem, with only two arms with really closed means :
\begin{verbatim}
Arm4=armBernoulli(0.39);
Arm5=armBernoulli(0.4);
\end{verbatim}

And here is what we obtained :
\begin{figure}[H]
	\centering
	\noindent\includegraphics[scale=0.7]{Q3-regrets-mypb.png}
	\caption{Regret curves for Thompson Sampling and Exp3 strategies on another problem}
\end{figure}

Here, at horizon 500, there is not a large difference between Exp3 and Thompson Sampling.


\end{document}