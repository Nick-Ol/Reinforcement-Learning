\documentclass[11pt,a4paper]{article}

\usepackage[applemac]{inputenc}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage[francais]{babel}
\usepackage{amsmath,amssymb}
\usepackage{pstricks,pst-plot}
\usepackage{calc}
\usepackage{multicol}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage[T1]{fontenc}
\usepackage[top=3.5cm, bottom=2.5cm, left=1.5cm, right=1.5cm]{geometry}
\usepackage{stmaryrd}
\usepackage{float}
\pagestyle{plain}
\usepackage{epstopdf}
\usepackage{stmaryrd} 


\title{TP 2 \\ The stochastic multi-armed bandit}
\author{Mathurin \textsc{Massias} \and Clement \textsc{Nicolle}}
\date{\today} 

\begin{document}
\maketitle

\section{Building a MAB problem}
\hspace{-6mm} We replaced the initial parameters by :
\begin{verbatim}
Arm1=armBernoulli(0.4); %mean 0.4
Arm2=armBeta(4,12); %mean 0.25
Arm3=armExp(9); %mean 0.11
Arm4=armFinite([0.2 0.4 0.7 0.8],[0.1 0.3 0.3 0.3]); %mean 0.59

MAB={Arm1,Arm2,Arm3,Arm4};
\end{verbatim}

\section{The UCB algorithm}

\hspace{-6mm} We implemented the naive strategy in \textit{naive.m}, simply by calling \textit{UCB.m} with parameter $\alpha = 0$.
%
\\[5mm]
\underline{Question 1 :} We plot the regret curves for the naive strategy and the UCB algorithm for several values of $\alpha$, logarithmically sampled from .01 to 1. We chose an horizon of 5000 draws of arms. We state that the final regret seems to be smaller for smaller values of $\alpha$. We will keep for $\alpha$ the value which gives the minimum final regret on a particular draw, so here we got $\alpha = 0.0464$.

\medskip
The Monte-Carlo estimations of the final regret at horizon 5000, with 100 trajectories simulated, are larger for the naive algorithm (around 100) than for the UCB with $\alpha = 0.0464$ (varying around 50). 
\\
As the variance of the regret must be quite high, MC estimations with 100 trajectories do not give the same results every time. What makes sense here is to observe that UCB gives a lower regret than the naive algorithm, but the values themselves are just an approximation of the mean regrets.
\\


\hspace*{-90mm}\begin{figure}[H]
	\hspace*{-15mm}\noindent\includegraphics[scale=0.6]{regret_curves.png}
	\caption{Regret curves for the naive strategy and the UCB algorithm for several values of $\alpha$}
\end{figure}

\section{Complexity of a bandit problem}

We take a new MAB only composed of Bernoulli arms :
\begin{verbatim}
Arm5 = armBernoulli(0.2);
Arm6 = armBernoulli(0.24);
Arm7 = armBernoulli(0.8);
MAB_easy ={Arm1, Arm7, Arm5, Arm6};
\end{verbatim}

For this MAB, we find as value for the complexity $c = 2,54$, which is low as the maximum mean is distant from the others.

We plot the lower bound and the regret curve :
\begin{figure}[H]
	\centering
	\noindent\includegraphics[scale=0.4]{complexity.jpg}
	\caption{Lower bound and regret curve}
\end{figure}

Here, we find effectively that the regret curve is above the lower bound. It might not be the case for another draw, as the result is asymptotic. Here, we stopped at n = 5000.

\section{A Bayesian idea for Bernoulli bandit problems}

"Optimism" consists in letting the agent play as if the environment was the most favorable among all environments that are still sufficiently likely given the observations accumulated so far.
\\
The UCB algorithm is optimistic because we pull the arm that has the greatest value that is statistically plausible at time t.
\\
Thompson sampling samples each action according to the probability it is optimal. So we cannot say this is an "optimistic algorithm".
\\[5mm]
\underline{Question 2 :} For the "easy" bandit problem with Bernoulli arms (the same as in section 2), here is what we find :

\begin{figure}[H]
	\centering
	\noindent\includegraphics[scale=0.4]{regret_thom_easy.jpg}
	\caption{Lower bound and regret curves for UCB algorithm and Thompson sampling, for an "easy" problem}
\end{figure}

We then implemented this for a more "difficult"proble, as the spread between the means is smaller :
\begin{verbatim}
ArmDiff1 = armBernoulli(0.22);
ArmDiff2 = armBernoulli(0.24);
ArmDiff3 = armBernoulli(0.23);
ArmDiff4 = armBernoulli(0.25);
MAB_diff ={ArmDiff1, ArmDiff2, ArmDiff3, ArmDiff4};
\end{verbatim}

We now have a much larger complexity : $c = 67.7$.
Below is the plot we obtained :
\begin{figure}[H]
	\centering
	\noindent\includegraphics[scale=0.4]{regret_thom_diff.jpg}
	\caption{Lower bound and regret curves for UCB algorithm and Thompson sampling, for a "difficult" problem}
\end{figure}

We immediately see that the horizon is not large enough to have the asymptotic result for the lower bound. For an horizon of 200000 pulls of arms, we got :


\begin{figure}[H]
	\centering
	\noindent\includegraphics[scale=0.4]{regret_thom_diff2.jpg}
	\caption{Lower bound and regret curves for UCB algorithm and Thompson sampling, for a "difficult" problem}
\end{figure}

On this example we have the lower bound.


\end{document}